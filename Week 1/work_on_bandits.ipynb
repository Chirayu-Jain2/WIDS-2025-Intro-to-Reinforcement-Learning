{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bandit is one option (or “arm”) you can choose, where the reward you get is uncertain and must be learned by trying it out.\n",
    "In multi-armed bandits, you repeatedly pick among several such uncertain options to find which one pays best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of ten bandit objects initialized in the list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [Bandit(random.random()*4-2) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate reward from that bandit, use the pullLever() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04364432528747145"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandits[0].pullLever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy(no_of_iterations):\n",
    "  avg_till_now=[0 for _ in range(10)]\n",
    "  reward_list=[]\n",
    "  for iterations in range(1,no_of_iterations+1):\n",
    "      reward=0\n",
    "      maximum=max(avg_till_now)\n",
    "      max_avg=[]\n",
    "      for j in range(10):\n",
    "          if avg_till_now[j]==maximum:\n",
    "              max_avg.append(j)\n",
    "      if len(max_avg)==1:\n",
    "          reward = bandits[max_avg[0]].pullLever()\n",
    "          reward_list.append(reward)\n",
    "          avg_till_now[max_avg[0]]+=(1/iterations) * (reward-avg_till_now[max_avg[0]])\n",
    "      \n",
    "      if len(max_avg)!=1:\n",
    "          selected=random.choice(max_avg)\n",
    "          reward = bandits[selected].pullLever()\n",
    "          reward_list.append(reward)\n",
    "          avg_till_now[selected]+=(1/iterations) * (reward-avg_till_now[selected])\n",
    "  return reward_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases. and display that image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epsilon_greedy(epsilon):\n",
    "    reward_list = []\n",
    "    reward=0\n",
    "    avg=[0]*10\n",
    "    is_epsilion = [True, False]\n",
    "    probablities = [epsilon, 1-epsilon]\n",
    "    times_chosen=[0]*10\n",
    "    for i in range(1,1001):\n",
    "        if(np.random.choice(is_epsilion, p=probablities)):\n",
    "            e_pull = random.randint(0,9)\n",
    "            reward = bandits[e_pull].pullLever()\n",
    "            times_chosen[e_pull]+=1\n",
    "            reward_list.append(reward)\n",
    "            avg[e_pull]+=1/times_chosen[e_pull] * (reward-avg[e_pull])\n",
    "        else:\n",
    "            greedy=max(avg)\n",
    "            greedy_list=[]\n",
    "            for j in range(10):\n",
    "                if avg[j]==greedy:\n",
    "                    greedy_list.append(j)\n",
    "            if len(greedy_list)==1:\n",
    "                reward=bandits[greedy_list[0]].pullLever()\n",
    "                reward_list.append(reward)\n",
    "                times_chosen[greedy_list[0]]+=1\n",
    "                avg[greedy_list[0]]+=1/times_chosen[greedy_list[0]] * (reward-avg[greedy_list[0]])\n",
    "            else:\n",
    "                select=random.choice(greedy_list)\n",
    "                reward=bandits[select].pullLever()\n",
    "                \n",
    "                times_chosen[select]+=1\n",
    "                avg[select]+=1/times_chosen[select] * (reward-avg[select])\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases but for various values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the $\\epsilon$-greedy algorithm for 1000 iterations and find the optimal $\\epsilon$ value by plotting the cumulative average of rewards for various values of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimistic_greedy():\n",
    "    # TODO: Implement the optimistic greedy algorithm here\n",
    "\n",
    "    # Return the reward from the bandits in a list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases for an optimistic greedy of $Q_1 = 10$ and a non-optimistic $\\epsilon = 0.1$ and try to compare which is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ucb(c):\n",
    "    # TODO: Implement the UCB algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
